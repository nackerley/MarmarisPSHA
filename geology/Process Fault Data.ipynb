{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openquake.hazardlib.geo.geodetic as geo\n",
    "import hmtk.sources as src\n",
    "import hmtk.plotting.seismicity.catalogue_plots as cp\n",
    "\n",
    "from string import Template\n",
    "from collections import OrderedDict\n",
    "from hmtk.plotting.mapping import HMTKBaseMap\n",
    "from hmtk.parsers.catalogue import CsvCatalogueParser\n",
    "from hmtk.parsers.source_model.nrml04_parser import nrmlSourceModelParser\n",
    "from hmtk.seismicity.selector import CatalogueSelector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alkan Hotel, Marmaris, Turkey\n",
    "target_lon = 28.2520426\n",
    "target_lat = 36.8429408\n",
    "\n",
    "# load fault data in JSON format\n",
    "source_file = \"Eastern_Mediterranean_Fault_Traces.geojson\"\n",
    "DATA = json.load(open(source_file, \"r\"))\n",
    "DATA = OrderedDict([(dat[\"properties\"][\"IDSOURCE\"], dat) \n",
    "                    for dat in DATA[\"features\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the code provided is good for printing fault data but nothing else\n",
    "def show_fault_properties(flt_id):\n",
    "    fault = DATA[flt_id]\n",
    "    # pretty print\n",
    "    print \"ID = %s\" % fault[\"properties\"][\"IDSOURCE\"]\n",
    "    print \"Name = %s\" % fault[\"properties\"][\"SOURCENAME\"]\n",
    "    print \"Trace (long. lat.):\"\n",
    "    for crd in fault[\"geometry\"][\"coordinates\"]:\n",
    "        print \"    %.6f %.6f\" % (crd[0], crd[1])\n",
    "    print \"Dip = %s - %s\" % (str(fault[\"properties\"][\"DIPMIN\"]), \n",
    "                             str(fault[\"properties\"][\"DIPMAX\"]))\n",
    "    print \"Rake = %s - %s\" % (str(fault[\"properties\"][\"RAKEMIN\"]), \n",
    "                              str(fault[\"properties\"][\"RAKEMAX\"]))\n",
    "    print \"Upper Seismogenic Depth = %s km\" % \\\n",
    "        (str(fault[\"properties\"][\"MINDEPTH\"]))\n",
    "    print \"Lower Seismogenic Depth = %s km\" % \\\n",
    "        (str(fault[\"properties\"][\"MAXDEPTH\"]))\n",
    "    print \"Slip Rate = %s - %s mm/yr\" % \\\n",
    "        (str(fault[\"properties\"][\"SRMIN\"]), \n",
    "         str(fault[\"properties\"][\"SRMAX\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID = TRCS394\n",
      "Name = Datca Horst North Boundary 1\n",
      "Trace (long. lat.):\n",
      "    27.653862 36.814250\n",
      "    27.772547 36.818590\n",
      "    27.891225 36.822930\n",
      "    28.009910 36.827270\n",
      "Dip = 60 - 60\n",
      "Rake = 288 - 288\n",
      "Upper Seismogenic Depth = 0.0 km\n",
      "Lower Seismogenic Depth = 13.0 km\n",
      "Slip Rate = 3.6056 - 7.2111 mm/yr\n"
     ]
    }
   ],
   "source": [
    "show_fault_properties('TRCS394') # a fault of interest because it is nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925 faults total\n",
      "Adding some statistics ...\n"
     ]
    }
   ],
   "source": [
    "# loop over faults; compute and append statistics of interest\n",
    "mu = 30e9 # [Pa] dummy value for estimate of moment rate\n",
    "print \"%d faults total\" % len(DATA)\n",
    "print \"Adding some statistics ...\"\n",
    "for fault in DATA:\n",
    "    fault_lonlat = np.array(DATA[fault][\"geometry\"][\"coordinates\"])\n",
    "    min_depth = np.array(DATA[fault][\"properties\"][\"MINDEPTH\"])\n",
    "    s_min = DATA[fault][\"properties\"][\"SRMIN\"]\n",
    "    s_max = DATA[fault][\"properties\"][\"SRMAX\"]\n",
    "    d_min = DATA[fault][\"properties\"][\"MINDEPTH\"]\n",
    "    d_max = DATA[fault][\"properties\"][\"MAXDEPTH\"]\n",
    "    phi_min = DATA[fault][\"properties\"][\"DIPMIN\"]\n",
    "    phi_max = DATA[fault][\"properties\"][\"DIPMAX\"]\n",
    "    \n",
    "    L_max = geo.distance(fault_lonlat[0,0], fault_lonlat[0,1], 0, \n",
    "                    fault_lonlat[-1,0], fault_lonlat[-1,1], 0)\n",
    "    R_all = geo.distance(target_lon, target_lat, 0, \n",
    "                    fault_lonlat[:,0], fault_lonlat[:,1], min_depth)\n",
    "    R_min = np.min(R_all)\n",
    "    R_max = np.max(R_all)\n",
    "    R_mid = (R_max + R_min)/2\n",
    "    W_max = (d_max - d_min)/np.abs(np.sin(phi_min))\n",
    "    W_mid = (d_max - d_min)/np.abs(np.sin((phi_max + phi_min)/2))\n",
    "    s_mid = (s_max + s_min)/2\n",
    "    L_mid = L_max/2\n",
    "    M0_rate_max = mu*L_max*1e3*W_max*1e3*s_max*1e-3\n",
    "    M0_rate_mid = mu*L_mid*1e3*W_mid*1e3*s_mid*1e-3\n",
    "    M0_rate_hybrid = mu*L_max*1e3*W_mid*1e3*s_mid*1e-3\n",
    "    danger = M0_rate_hybrid/R_min\n",
    "    \n",
    "    DATA[fault][\"statistics\"] = {\n",
    "        \"R_min\": R_min,\n",
    "        \"R_max\": R_max,\n",
    "        \"R_mid\": R_mid,\n",
    "        \"L_max\": L_max,\n",
    "        \"W_max\": W_max,\n",
    "        \"W_mid\": W_mid,\n",
    "        \"s_mid\": s_mid,\n",
    "        \"M0_rate_max\": M0_rate_max,\n",
    "        \"M0_rate_mid\": M0_rate_mid,\n",
    "        \"M0_rate_hybrid\": M0_rate_mid,\n",
    "        \"danger\": danger,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a function for filtering the fault list by \"danger\"\n",
    "def pick_dangerous_faults(data, key, n, quiet=False):\n",
    "    \n",
    "    if key == 'R_min':\n",
    "        reverse = False\n",
    "    else:\n",
    "        reverse = True\n",
    "        \n",
    "    data_sorted = sorted(data.items(), \n",
    "                key=lambda item: item[1][\"statistics\"][key], \n",
    "                                     reverse=reverse)\n",
    "\n",
    "    top = OrderedDict(data_sorted[:n])\n",
    "\n",
    "    if not quiet:\n",
    "        print \"  fault, R_min, L_max, W_mid, s_mid, M0_rate_hybrid, danger?\"\n",
    "    for (key, fault) in top.items():\n",
    "        s_max = fault[\"properties\"][\"SRMAX\"]\n",
    "\n",
    "        R_min = fault[\"statistics\"][\"R_min\"]\n",
    "        L_max = fault[\"statistics\"][\"L_max\"]\n",
    "        W_mid = fault[\"statistics\"][\"W_mid\"]\n",
    "        s_mid = fault[\"statistics\"][\"s_mid\"]\n",
    "        M0_rate_max = fault[\"statistics\"][\"M0_rate_max\"]\n",
    "        M0_rate_mid = fault[\"statistics\"][\"M0_rate_mid\"]\n",
    "        M0_rate_hybrid = fault[\"statistics\"][\"M0_rate_hybrid\"]\n",
    "        danger = fault[\"statistics\"][\"danger\"]\n",
    "\n",
    "        if not quiet:\n",
    "            print \"%s, %5.1f, %5.1f, %5.1f, %5.1f, %7.1e, %7.1e\"  % \\\n",
    "                (key, R_min, L_max, W_mid, s_mid, M0_rate_hybrid, danger)\n",
    "        \n",
    "    return top"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "closest_faults = pick_dangerous_faults(DATA, 'R_min', 15);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "most_dangerous_faults = pick_dangerous_faults(DATA, 'danger', 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\\'1.0\\' encoding=\\'utf-8\\'?>\\n<nrml xmlns:gml=\"http://www.opengis.net/gml\"\\n      xmlns=\"http://openquake.org/xmlns/nrml/0.4\">\\n\\n    <sourceModel name=\"$SOURCEMODELNAME\">\\n\\n        <simpleFaultSource id=\"$IDSOURCE\" name=\"$SOURCENAME\" tectonicRegion=\"$REGION\">\\n            <simpleFaultGeometry>\\n                <gml:LineString>\\n                    <gml:posList>\\n$TRACE\\n                    </gml:posList>\\n                </gml:LineString>\\n                <dip>$DIP</dip>\\n                <upperSeismoDepth>$MINDEPTH</upperSeismoDepth>\\n                <lowerSeismoDepth>$MAXDEPTH</lowerSeismoDepth>\\n            </simpleFaultGeometry>\\n            <magScaleRel>$SCALEREL</magScaleRel>\\n            <ruptAspectRatio>$ASPECTRATIO</ruptAspectRatio>\\n            <$MFD/>\\n            <rake>$RAKE</rake>\\n        </simpleFaultSource>\\n\\n    </sourceModel>\\n</nrml>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_file = \"SimpleFaultTemplate.xml\" \n",
    "with open(template_file, 'r') as myfile:\n",
    "    template_string = myfile.read()\n",
    "template_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fault_json_to_xml(fault_json):\n",
    "\n",
    "    fault_file_list = []\n",
    "    region = \"Active Shallow Crust\"\n",
    "    coord_format = \"                        %10.6f %10.6f\\n\"\n",
    "    scaling_relation = \"WC1994\"\n",
    "    aspect_ratio = 2.0\n",
    "    MFD = 'truncGutenbergRichterMFD aValue=\"4.0\" bValue=\"1.0\" minMag=\"5.0\" maxMag=\"8.0\"'\n",
    "\n",
    "    for (key, fault) in fault_json.items():\n",
    "\n",
    "        fault_lonlat = np.array(fault[\"geometry\"][\"coordinates\"])\n",
    "        coord_string = [(coord_format % tuple(coords)) for coords in fault_lonlat]\n",
    "        coord_string = (''.join(coord_string))[:-1]\n",
    "        dip_min = fault[\"properties\"][\"DIPMIN\"]\n",
    "        dip_max = fault[\"properties\"][\"DIPMAX\"]\n",
    "        rake_min = fault[\"properties\"][\"RAKEMIN\"]\n",
    "        rake_max = fault[\"properties\"][\"RAKEMAX\"]\n",
    "\n",
    "        with open(template_file, 'r') as myfile:\n",
    "            template_string = myfile.read()\n",
    "\n",
    "        template = Template(template_string)\n",
    "        output_string = template.substitute(\n",
    "            IDSOURCE=key,\n",
    "            SOURCEMODELNAME=\"$SOURCEMODELNAME\",\n",
    "            SOURCENAME=fault[\"properties\"][\"SOURCENAME\"],\n",
    "            REGION=region,\n",
    "            TRACE=coord_string,\n",
    "            DIP=\"%.1f\" % ((dip_min + dip_max)/2),\n",
    "            MINDEPTH=\"%.1f\" % fault[\"properties\"][\"MINDEPTH\"],\n",
    "            MAXDEPTH=\"%.1f\" % fault[\"properties\"][\"MAXDEPTH\"],\n",
    "            SCALEREL=scaling_relation,\n",
    "            ASPECTRATIO=\"%.3g\" % aspect_ratio,\n",
    "            MFD=MFD,\n",
    "            RAKE=\"%.1f\" % ((rake_min + rake_max)/2),\n",
    "        )\n",
    "\n",
    "        output_file = key + '.xml'\n",
    "\n",
    "        with open(output_file, 'w') as myfile:\n",
    "            myfile.write(output_string)\n",
    "\n",
    "        fault_file_list.append(output_file)\n",
    "                \n",
    "    return fault_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def concat_xml_faults(fault_xml_list, source_model_name, output_xml):\n",
    "    \n",
    "    joint_string = '\\n    </sourceModel>\\n' \\\n",
    "        + '</nrml>\\n<?xml version=\\'1.0\\' encoding=\\'utf-8\\'?>\\n' \\\n",
    "        + '<nrml xmlns:gml=\"http://www.opengis.net/gml\"\\n' \\\n",
    "        + '      xmlns=\"http://openquake.org/xmlns/nrml/0.4\">\\n\\n' \\\n",
    "        + '    <sourceModel name=\"$SOURCEMODELNAME\">\\n'\n",
    "\n",
    "    # read and concatenate all of the files\n",
    "    faults = ''\n",
    "    for file_name in fault_xml_list:\n",
    "        with open(file_name, 'r') as myfile:\n",
    "            fault = myfile.read()\n",
    "        faults = faults + fault\n",
    "\n",
    "    # remove redundent start/end XML \n",
    "    faults = faults.replace(joint_string,'')\n",
    "\n",
    "    # give the model a name\n",
    "    template = Template(faults)\n",
    "    output_string = template.substitute(\n",
    "        SOURCEMODELNAME=source_model_name)\n",
    "\n",
    "    # ship it\n",
    "    print \"Writing results to \", output_xml\n",
    "    with open(output_xml, 'w') as myfile:\n",
    "        myfile.write(output_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# other faults to add\n",
    "subduction_fault_file = \"HellenicArc.xml\"\n",
    "n_faults = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fault, R_min, L_max, W_mid, s_mid, M0_rate_hybrid, danger?\n",
      "TRCS996, 425.3, 271.5,  74.9,  27.7, 8.4e+18, 4.0e+16\n",
      "TRCS394,  21.6,  31.7,  42.6,   5.4, 1.1e+17, 1.0e+16\n",
      "TRCS006, 623.5, 293.0,  26.6,  25.0, 2.9e+18, 9.4e+15\n",
      "TRCS391,  23.1,  49.6,  15.7,   9.0, 1.1e+17, 9.1e+15\n",
      "TRCS331, 116.3, 119.9,  49.5,   4.3, 3.8e+17, 6.6e+15\n",
      "TRCS271,  77.7,  38.9,  57.2,   7.4, 2.5e+17, 6.4e+15\n",
      "TRCS322, 157.5, 114.0,  49.5,   5.7, 4.8e+17, 6.1e+15\n",
      "GRCS999, 427.1,  60.1,  51.1,  28.0, 1.3e+18, 6.0e+15\n",
      "TRCS003, 773.4, 353.7,  18.3,  22.4, 2.2e+18, 5.6e+15\n",
      "TRCS393,  48.5,  21.2,  42.6,   9.8, 1.3e+17, 5.5e+15\n",
      "TRCS013, 438.9,  99.1,  27.8,  27.0, 1.1e+18, 5.1e+15\n",
      "TRCS007, 499.3, 165.1,  20.7,  24.2, 1.2e+18, 5.0e+15\n",
      "TRCS390, 239.7, 254.4,  38.9,   3.9, 5.8e+17, 4.9e+15\n",
      "TRCS268,  85.4,  36.3,  49.5,   7.4, 2.0e+17, 4.7e+15\n",
      "GRCS603, 706.4, 116.5,  68.9,  12.5, 1.5e+18, 4.3e+15\n",
      "TRCS913,  21.3,  84.2,  19.6,   1.8, 4.3e+16, 4.1e+15\n",
      "TRCS023, 397.5,  95.0, 102.2,   5.3, 7.8e+17, 3.9e+15\n",
      "TRCS001, 1111.9,  51.2, 102.2,  26.0, 2.0e+18, 3.7e+15\n",
      "TRCS133, 893.9, 175.3, 161.8,   3.8, 1.6e+18, 3.7e+15\n",
      "TRCS330, 139.9,  39.1,  49.5,   8.6, 2.5e+17, 3.6e+15\n",
      "Writing results to  MarmarisDangerousSources21.xml\n"
     ]
    }
   ],
   "source": [
    "# write \"dangerous\" source model\n",
    "dangerous_sources_file = \"MarmarisDangerousSources%d.xml\" \\\n",
    "    % (n_faults + 1)\n",
    "sources_name = \"Top %d sources potentially dangerous to Marmaris\" \\\n",
    "    % (n_faults + 1)\n",
    "\n",
    "faults = pick_dangerous_faults(DATA, 'danger', n_faults, quiet=False)\n",
    "fault_file_list = fault_json_to_xml(faults)\n",
    "fault_file_list.append(subduction_fault_file)\n",
    "\n",
    "concat_xml_faults(fault_file_list, sources_name, dangerous_sources_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fault, R_min, L_max, W_mid, s_mid, M0_rate_hybrid, danger?\n",
      "TRCS913,  21.3,  84.2,  19.6,   1.8, 4.3e+16, 4.1e+15\n",
      "TRCS394,  21.6,  31.7,  42.6,   5.4, 1.1e+17, 1.0e+16\n",
      "TRCS391,  23.1,  49.6,  15.7,   9.0, 1.1e+17, 9.1e+15\n",
      "TRCS396,  38.4,  51.9,  13.1,   5.5, 5.6e+16, 2.9e+15\n",
      "TRCS413,  40.1,  31.2,  13.1,   1.0, 6.3e+15, 3.1e+14\n",
      "TRCS414,  41.2,  15.9,  13.1,   1.0, 3.2e+15, 1.6e+14\n",
      "TRCS392,  43.3,  16.4,  16.8,   4.9, 2.0e+16, 9.4e+14\n",
      "TRCS412,  48.0,  11.5,  13.1,   1.0, 2.3e+15, 9.7e+13\n",
      "TRCS393,  48.5,  21.2,  42.6,   9.8, 1.3e+17, 5.5e+15\n",
      "TRCS397,  50.6,  22.0,  13.1,   1.0, 4.4e+15, 1.8e+14\n",
      "TRCS415,  58.8,  38.6,  13.1,   1.0, 7.8e+15, 2.7e+14\n",
      "TRCS398,  59.0,  10.1,  13.1,   1.0, 2.1e+15, 7.0e+13\n",
      "TRCS395,  63.3,  12.3,  42.6,   5.4, 4.2e+16, 1.3e+15\n",
      "TRCS338,  70.1,  17.1,  39.4,   5.8, 5.9e+16, 1.7e+15\n",
      "TRCS271,  77.7,  38.9,  57.2,   7.4, 2.5e+17, 6.4e+15\n",
      "GRCS965,  77.8,  44.2,  19.6,   3.0, 3.9e+16, 1.0e+15\n",
      "TRCS293,  78.6,  22.9,  53.4,   1.3, 2.4e+16, 6.1e+14\n",
      "TRCS268,  85.4,  36.3,  49.5,   7.4, 2.0e+17, 4.7e+15\n",
      "TRCS267,  85.8,   9.0,  57.2,   7.8, 6.0e+16, 1.4e+15\n",
      "TRCS289,  86.7,  47.4,  45.9,   1.3, 4.1e+16, 9.5e+14\n",
      "Writing results to  MarmarisNearbySources21.xml\n"
     ]
    }
   ],
   "source": [
    "# write \"nearby\" source model\n",
    "nearby_sources_file = \"MarmarisNearbySources%d.xml\" \\\n",
    "    % (n_faults + 1)\n",
    "sources_name = \"Top %d sources nearest to Marmaris.xml\" \\\n",
    "    % (n_faults + 1)\n",
    "\n",
    "faults = pick_dangerous_faults(DATA, 'R_min', n_faults, quiet=False)\n",
    "fault_file_list = fault_json_to_xml(faults)\n",
    "fault_file_list.append(subduction_fault_file)\n",
    "\n",
    "concat_xml_faults(fault_file_list, sources_name, nearby_sources_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalogue Attribute  is not a recognised catalogue key\n",
      "Catalogue Attribute Identifier is not a recognised catalogue key\n",
      "Minimum magnitude:  0.75\n",
      "Maximum magnitude:  7.6\n",
      "Number of events:  30006\n",
      "Catalogue keys: \n",
      "['eventID', 'comment', 'sigmaMagnitude', 'hour', 'SemiMinor90', 'magnitude', 'Agency', 'second', 'longitude', 'month', 'depthError', 'flag', 'depth', 'magnitudeType', 'SemiMajor90', 'timeError', 'year', 'latitude', 'ErrorStrike', 'day', 'minute']\n"
     ]
    }
   ],
   "source": [
    "catalogue_filename = '../seismicity/output_homogenized_Marmaris.csv'\n",
    "parser = CsvCatalogueParser(catalogue_filename) # From .csv to hmtk\n",
    "\n",
    "# Read and process the catalogue content in a variable called \"catalogue\"\n",
    "catalogue = parser.read_file() \n",
    "\n",
    "print 'Minimum magnitude: ', np.min(catalogue.data['magnitude'])\n",
    "print 'Maximum magnitude: ', np.max(catalogue.data['magnitude'])\n",
    "print 'Number of events: ', len(catalogue.data['magnitude'])\n",
    "print 'Catalogue keys: '\n",
    "print catalogue.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9557dd752ea3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m depth_hist = catalogue.get_depth_distribution(depth_bins,\n\u001b[0;32m      9\u001b[0m                                           \u001b[0mnormalisation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormalisation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                                           bootstrap=bootstrap)\n\u001b[0m\u001b[0;32m     11\u001b[0m plt.bar(depth_bins[:-1],\n\u001b[0;32m     12\u001b[0m         \u001b[0mdepth_hist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nick/src/python/GEM/hmtk/hmtk/seismicity/catalogue.pyc\u001b[0m in \u001b[0;36mget_depth_distribution\u001b[1;34m(self, depth_bins, normalisation, bootstrap)\u001b[0m\n\u001b[0;32m    322\u001b[0m                                       \u001b[0mnormalisation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormalisation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                                       \u001b[0mnumber_bootstraps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                                       boundaries=(0., None))\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_depth_pmf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_bins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nick/src/python/GEM/hmtk/hmtk/seismicity/utils.pyc\u001b[0m in \u001b[0;36mbootstrap_histogram_1D\u001b[1;34m(values, intervals, uncertainties, normalisation, number_bootstraps, boundaries)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# No bootstraps or all uncertaintes are zero - return ordinary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;31m# histogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintervals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnormalisation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/src/python/anaconda3/envs/py27/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mhistogram\u001b[1;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             n += np.r_[sa.searchsorted(bins[:-1], 'left'), \\\n\u001b[1;32m--> 199\u001b[1;33m                 sa.searchsorted(bins[-1], 'right')]\n\u001b[0m\u001b[0;32m    200\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mzero\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mntype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of bounds"
     ]
    }
   ],
   "source": [
    "bin_width = 10\n",
    "normalisation=False\n",
    "bootstrap=None\n",
    "depth_bins = np.arange(0.,\n",
    "                       np.max(catalogue.data['depth']) + bin_width,\n",
    "                       bin_width)\n",
    "\n",
    "depth_hist = catalogue.get_depth_distribution(depth_bins,\n",
    "                                          normalisation=normalisation,\n",
    "                                          bootstrap=bootstrap)\n",
    "plt.bar(depth_bins[:-1],\n",
    "        depth_hist,\n",
    "        width=0.9*bin_width,\n",
    "        edgecolor='k')\n",
    "plt.xlabel('Depth (km)', fontsize='large')\n",
    "if normalisation:\n",
    "    plt.ylabel('Probability Mass Function', fontsize='large')\n",
    "else:\n",
    "    plt.ylabel('Count')\n",
    "plt.title('Depth Histogram', fontsize='large')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlim((0,150))\n",
    "plt.yscale('log')\n",
    "plt.ylim((10,1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cp.plot_magnitude_time_density(catalogue, 0.2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeness_table_a = np.array([[1992, 3.5],\n",
    "                                 [1978, 4.0], \n",
    "                                 [1963, 5.0],\n",
    "                                 [1900, 7.0]]) \n",
    "cp.plot_observed_recurrence(catalogue, completeness_table_a, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map configuration\n",
    "span = 1 # deg\n",
    "map_config = {\"min_lon\": target_lon - 2*span, \n",
    "              \"max_lon\": target_lon + 2*span, \n",
    "              \"min_lat\": target_lat - span, \n",
    "              \"max_lat\": target_lat + span, \"resolution\": \"l\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a basemap \n",
    "basemap1 = HMTKBaseMap(map_config, 'Catalogue and nearest sources')\n",
    "\n",
    "# Adding the catalogue to the basemap\n",
    "basemap1.add_catalogue(catalogue, overlay=True)\n",
    "\n",
    "# Reading the models \n",
    "parser = nrmlSourceModelParser(nearby_sources_file)\n",
    "\n",
    "# Parse the seismic sources and save them in \"source_model\"\n",
    "source_model = parser.read_file(\"Sources Around Marmaris\")\n",
    "\n",
    "# Adding the seismic sources\n",
    "basemap1.add_source_model(source_model, area_border='r-', \n",
    "                          border_width=1.5, alpha=0.5, overlay=True)\n",
    "\n",
    "# Add target\n",
    "basemap1.add_size_scaled_points(target_lon, target_lat, 20, shape='*', \n",
    "                                colour='k', zorder=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a basemap \n",
    "basemap1 = HMTKBaseMap(map_config, 'Catalogue and \"most dangerous\" sources')\n",
    "\n",
    "# Adding the catalogue to the basemap\n",
    "basemap1.add_catalogue(catalogue, overlay=True)\n",
    "\n",
    "# Reading the models \n",
    "parser = nrmlSourceModelParser(dangerous_sources_file)\n",
    "\n",
    "# Parse the seismic sources and save them in \"source_model\"\n",
    "source_model = parser.read_file(\"Sources Around Marmaris\")\n",
    "\n",
    "# Adding the seismic sources\n",
    "basemap1.add_source_model(source_model, area_border='r-', \n",
    "                          border_width=1.5, alpha=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selector = CatalogueSelector(catalogue, create_copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for source in source_model.sources:\n",
    "    if isinstance(source, src.area_source.mtkAreaSource): \n",
    "        source.select_catalogue(selector)\n",
    "        print 'Area source %s, name %s, # of events %8.0f' % (\n",
    "            source.id, source.name, source.catalogue.get_number_events())\n",
    "        #subcatalogue_area = selector.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_km = 10*np.ones_like(source_model.sources)\n",
    "d_km[-1] = 30\n",
    "for i, source in enumerate(source_model.sources):\n",
    "    if isinstance(source, src.simple_fault_source.mtkSimpleFaultSource): \n",
    "        source.select_catalogue(selector, d_km[i])\n",
    "    elif isinstance(source, src.complex_fault_source.mtkComplexFaultSource): \n",
    "        source.select_catalogue(selector, d_km[i]) \n",
    "        \n",
    "    print '%s: %s, %d events wihin %g km' % (\n",
    "        source.id, source.name, source.catalogue.get_number_events(), d_km[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, source in enumerate(source_model.sources):\n",
    "    n_events = source.catalogue.get_number_events()\n",
    "    if n_events < 5: continue\n",
    "    basemap = HMTKBaseMap(\n",
    "        map_config, '%d events %d km from %s' % (n_events, d_km[i], source.name))\n",
    "    basemap.add_catalogue(source.catalogue, overlay=True)\n",
    "    basemap.add_source_model(source_model, area_border='k-')\n",
    "    \n",
    "    cp.plot_observed_recurrence(source.catalogue, completeness_table_a, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
